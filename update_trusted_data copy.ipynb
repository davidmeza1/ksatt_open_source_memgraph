{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refresh KSATT Open Source Graph Database\n",
    "\n",
    "### Usage\n",
    "\n",
    "Neo4j Version 4.4.5 was used at time of updating this.\n",
    "\n",
    "While all of these sources are relatively static, they do have version updates (on irregular cadences).  \n",
    "It is not urgent to incorporate these updates, so running these updates should be done monthly or quarterly and require a more significant rebuild of the KSATT graph database as well.  \n",
    "\n",
    "Prior to running this code for the first time, you should:\n",
    "- Ensure you have all python packages installed on your local machine (and optionally in the virtual environment you use to run this).\n",
    "- Create an empty new local Neo4j graph database. \n",
    "  - Start it upon creation, then stop it. \n",
    "  - Install the APOC plugin. \n",
    "  - Edit the settings file so that the minimum and maximum heap sizes are at least 1GB and 3GB respectively.\n",
    "  - You'll need to restart the database after making those changes to the configuration.\n",
    "- Substitute in your file path to the `neo4j_import_folder` variable in the *Prep* codeblock under *Download Data*. \n",
    "  - This file path can be found through Neo4j Desktop (... > Open folder > DBMS > import). I will open this directory in Finder then select \"New Terminal at Folder\" and `pwd` to copy and paste the full path. \n",
    "- Manually copy the `opm_onet_crosswalk.csv` file from this repository into the Neo4j DBMS import folder. \n",
    "\n",
    "Make sure your Neo4j database is started before running this code.  \n",
    "When this code is ran, it:\n",
    "- Downloads data from ONET, ESCO and NICE in the form of CSVs into the Neo4j import folder.\n",
    "- Cleans all data in the Neo4j import folder in preparation for upload. \n",
    "- Clears a local **existing and running** Neo4j graph database, then uploads the newly downloaded and cleaned data. \n",
    "\n",
    "### To Do\n",
    "- Incorporate [ONET Crosswalks](https://www.onetcenter.org/crosswalks.html).\n",
    "- Possibly incorporate USA Jobs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from py2neo import Graph\n",
    "import platform\n",
    "import os\n",
    "from fuzzywuzzy import process\n",
    "import getpass\n",
    "import cleaner # cleaner.py\n",
    "\n",
    "# Must copy data files to the appropriate neo4j folder. \n",
    "# NOTE: You must change this according to your environment.\n",
    "neo4j_import_folder = \"/Users/mgipson1/Library/Application Support/Neo4j Desktop/Application/relate-data/dbmss/dbms-1c2ec74c-9140-4893-86b3-bdf31fb91199/import\"\n",
    "\n",
    "download_new_data = False\n",
    "\n",
    "folders = [\"onet\", \"esco\", \"nice\"]\n",
    "for f in folders:\n",
    "    full_path = os.path.join(neo4j_import_folder, f)\n",
    "    if not os.path.exists(full_path):\n",
    "        os.mkdir(full_path)\n",
    "\n",
    "### FOR GRAPH ###\n",
    "\n",
    "def interrupt(query, message):\n",
    "    \"\"\"This function is for logging purposes.\"\"\"\n",
    "    raise Exception('ERROR! Could not finish query:', query, 'because', message)\n",
    "\n",
    "def run_apoc_query(query):\n",
    "    \"\"\"\n",
    "    This function is for easily running queries on the Neo4j database with Py2neo functions.\n",
    "    It also has the ability to catch issues with queries not being able to complete, \n",
    "    which the basic \"run\" function doesn't do on it's own. \n",
    "    \"\"\"\n",
    "\n",
    "    g = graph.begin() # open transaction\n",
    "    result = g.run(query).to_data_frame() # execute query and show results as a data frame\n",
    "    # NOTE: SOMETIMES I'VE BEEN GETTING AN INDEXERROR - THIS SEEMS TO BE RESOLVED AFTER STOPPING/RESTARTING THE DATABASE FOR THE MOST PART\n",
    "    # NOTE: SOMETIMES IT SEEMS TO BE WHEN THERE'S TOO MUCH LOAD IN A QUERY - EG MERGING THINGS WHERE TITLES AREN'T UNIQUE\n",
    "\n",
    "    # if the result of the executed query results in 1+ failed operations, STOP the script - if it's fine, commit changes to database\n",
    "    try:\n",
    "        if result['operations'][0]['failed'] > 0:\n",
    "            interrupt(query, \"FAILED OPERATIONS\\n",
    "        else:\n",
    "            graph.commit(g) # close transaction\n",
    "            print('Completed query.')\n",
    "    except Exception as e:\n",
    "        print(\"Did not complete query because we encountered an error.\\n",
    "        print(e)\n",
    "        interrupt(query, \"Error with APOC query.\\n",
    "    return result\n",
    "\n",
    "def run_reg_query(query):\n",
    "    \"\"\"\n",
    "    This function is for easily running queries on the Neo4j database with Py2neo functions.\n",
    "    It also has the ability to catch issues with queries not being able to complete, \n",
    "    which the basic \"run\" function doesn't do on it's own. \n",
    "    \"\"\"\n",
    "    # print(graph)\n",
    "    g = graph.begin() # open transaction\n",
    "    # print(\"graph begin\\n",
    "    result = g.run(query).to_data_frame() # execute query and show results as a data frame\n",
    "    # print(\"query ran\\n",
    "    graph.commit(g) # close transaction\n",
    "    # print(graph)\n",
    "    return result\n",
    "\n",
    "# Connect to local Neo4j instance\n",
    "neo4j_port = \"7687\"\n",
    "neo4j_uid = getpass.getpass(\"Username:\ # default is neo4j\n",
    "neo4j_pwd = getpass.getpass(\"Password:\\n",
    "\n",
    "try:\n",
    "    graph = Graph('bolt://localhost:'+neo4j_port, auth=(neo4j_uid, neo4j_pwd))\n",
    "    print('Connected to the graph database!')\n",
    "except Exception as e:\n",
    "    print('ERROR! Could not connect to the Neo4j database. See console for details.')\n",
    "    raise SystemExit(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Establishing other functions.\\n",
    "\n",
    "def xlsx_to_csv(xlsx_name, file_path, header_bool=False, fillna=True):\n",
    "    print(\"Converting \"+xlsx_name+\" in \"+file_path+\" to a CSV\\n",
    "\n",
    "    # excel to dataframe where na values resulting from merged cells are filled\n",
    "    # read the XLSX file (indicated with the file path + file name) using pandas\n",
    "    # have to indicate dtype=str because it will read numbers as integers and result in truncation/conversion issues\n",
    "    # have to indicate engine=\"openpyxl\" because otherwise it tries to use the xlrd library that's deprecated (?) and has issues\n",
    "    xlsx_file = pd.read_excel(os.path.join(file_path, xlsx_name), dtype=str, engine=\"openpyxl\\n",
    "\n",
    "    # \"fillna\" is True by default in the function parameters, which means by default we'll replace null values with valid ones\n",
    "    # but on occasion this might not be the case, so we have an option to skip the cleaning process\n",
    "    if fillna:\n",
    "        df = xlsx_file.fillna(method=\"ffill\ # fill the dataframe in a way that makes sense - pandas takes care of this for us with the ffill method\n",
    "        na_rows = (df.index[df.iloc[:,0].isna()]).to_list() # find the indicies of all na values in first column\n",
    "        df.drop(na_rows, inplace=True) # then drop those rows\n",
    "    else:\n",
    "        df = xlsx_file # don't do anything to the dataframe\n",
    "\n",
    "    # write the dataframe to a CSV with the same name as the XLSX file \n",
    "    csv_file = os.path.join(file_path, xlsx_name.replace(\"xlsx\", \"csv\)\n",
    "    df.to_csv(csv_file, index=False, header=header_bool) # don't want indices, and by default don't want headers either\n",
    "\n",
    "    # remove the old XLSX file that you just converted to a clean CSV\n",
    "    os.remove(os.path.join(file_path, xlsx_name))\n",
    "\n",
    "    print(\"Converted \"+xlsx_name+\" to a CSV and removed the original XLSX file\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_new_data:\n",
    "    print(\"Scrape the ONET database online for new files and download them or overwrite older files.\\n",
    "    folder = os.path.join(neo4j_import_folder, \"onet\\n",
    "\n",
    "    # tell which files we need\n",
    "    onet_files = [\n",
    "            \"abilities\", \"alternatetitles\",\n",
    "            \"contentmodelreference\",\n",
    "            \"dwareference\",\n",
    "            \"educationtrainingandexperience\", \"educationtrainingandexperiencecategories\",\n",
    "            \"interests\",\n",
    "            \"knowledge\",\n",
    "            \"occupationdata\",\n",
    "            \"scalesreference\", \"skills\",\n",
    "            \"taskratings\", \"taskstatements\", \"taskstodwas\",\n",
    "            \"technologyskills\", \"toolsused\",\n",
    "            \"unspscreference\",\n",
    "            \"workactivities\", \"workcontext\", \"workstyles\", \"workvalues\"\n",
    "    ]\n",
    "\n",
    "    print(\"Grab files from the main ONET 'all files' page\\n",
    "\n",
    "    # Validate ONET website URL, then process html doc from url\n",
    "    url = 'https://www.onetcenter.org/database.html#all-files'\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('ERROR: Could not access ONET Database URL. See console for exception details.')\n",
    "        raise SystemExit(e)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Scrape page for all links\n",
    "    for link in soup.find_all('a'):\n",
    "        # if it's an href attribute & contains .xlsx, consider it\n",
    "        if 'href' in link.attrs:\n",
    "            if '.xlsx' in link.attrs['href']:\n",
    "                print(\"Processing an XLSX file found on the ONET page\\n",
    "                # extract the specific file_url and get the html of that specific .xlsx file page (aka file contents) by appending to the base ONET url\n",
    "                file_url = link.attrs['href']\n",
    "                response = requests.get('https://www.onetcenter.org' + file_url)\n",
    "                # create a sanitized xlsx_name\n",
    "                xlsx_name = re.search('[^/]+$', file_url).group(0)\n",
    "                for i in ['%20', '%2C', '-', ',', '_']:\n",
    "                    xlsx_name = xlsx_name.replace(i, '')\n",
    "                xlsx_name = xlsx_name.lower()\n",
    "                if xlsx_name.replace('.xlsx', '') in onet_files:\n",
    "                    # download xlsx, convert to csv, remove xlsx\n",
    "                    open(os.path.join(folder, xlsx_name), 'wb').write(response.content)\n",
    "                    xlsx_to_csv(xlsx_name, folder, True)\n",
    "\n",
    "    print(\"Now grab Occupational Listings zip that has additional files\\n",
    "    more_onet_files = ['detailed.csv', 'soc_structure.csv']\n",
    "\n",
    "    # download zip to the current directory\n",
    "    url = 'https://www.onetcenter.org/dl_files/OccupationalListings.zip'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    zip_path = os.path.join(folder, 'onet-bundle.zip')\n",
    "    open(zip_path, 'wb').write(r.content)\n",
    "\n",
    "    # extract files from zip bundle\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(folder))\n",
    "\n",
    "    unzipped_folder_path = os.path.join(folder, 'OccupationalListings')\n",
    "    # go through all folders under OccupationalListings\n",
    "    for sub_folder in os.listdir(unzipped_folder_path):\n",
    "        # go through all files in each *relevant* folder and move them to the import folder, also converting to CSV\n",
    "        sub_folder_path = os.path.join(unzipped_folder_path, sub_folder)\n",
    "        if 'Data_Level' in sub_folder or 'Structure' in sub_folder:\n",
    "            for file_name in os.listdir(sub_folder_path):\n",
    "                # get xlsx name and csv name\n",
    "                xlsx_name = file_name.replace(' ', '').lower()\n",
    "                csv_name = xlsx_name.replace('xlsx', 'csv')\n",
    "                if csv_name in more_onet_files:\n",
    "                    # get old location, assign new name and location\n",
    "                    old_loc = os.path.join(sub_folder_path, file_name) # sub_folder_path and file_name are import/ONET/OccupationListings/2019_folder/File_2019.xlsx\n",
    "                    new_loc = os.path.join(folder, xlsx_name) # onet_folder and xlsx_name are import/ONET/file02019.xlsx\n",
    "                    # essentially move the file and convert to CSV\n",
    "                    os.rename(old_loc, new_loc) \n",
    "                    xlsx_to_csv(xlsx_name, folder, fillna=False)\n",
    "                    # for some reason, all these files have a weird couple first rows, so we just want to remove those\n",
    "                    df = pd.read_csv(os.path.join(folder, csv_name), skiprows=2)\n",
    "                    df.to_csv(os.path.join(folder, csv_name), index=False)\n",
    "                else:\n",
    "                    os.remove(os.path.join(sub_folder_path, file_name))\n",
    "        # if the folder is irrelevant then remove all the files so the empty folder can get deleted with no issue\n",
    "        else: \n",
    "            for file_name in os.listdir(sub_folder_path):\n",
    "                os.remove(os.path.join(sub_folder_path, file_name))\n",
    "        # remove the sub-folders and main folder from the current directory since everything moved to the import folder\n",
    "        os.rmdir(sub_folder_path)\n",
    "    os.rmdir(unzipped_folder_path)\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    # fill one of the CSV files appropriately\n",
    "    file_path = os.path.join(folder, \"soc_structure.csv\\n",
    "    csv_file = pd.read_csv(file_path, dtype=str)\n",
    "    df = csv_file.fillna(method='ffill')\n",
    "    na_rows = (df.index[df.iloc[:,0].isna()]).to_list() \n",
    "    df.drop(na_rows, inplace=True)\n",
    "    df.to_csv(file_path, index=False, header=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_new_data:\n",
    "    print(\"Scrape the ESCO website/package for new files and download them or overwrite older files.\\n",
    "    folder = os.path.join(neo4j_import_folder, \"esco\\n",
    "\n",
    "    # URLs that facilitate downloadAction\n",
    "    # https://ec.europa.eu/esco/portal/downloadRequest/9e75dade-397d-4fe4-ba3a-20cac74820f0,091f7d25-07c7-4a72-a673-d9b765d7db08,946edf49-3609-470c-8505-febf3dd72f84,09f931c8-2984-4e5e-b049-8e7c466bab04,215b7cb4-9818-4f0e-8d0d-7fdea1b83b9f,a9ccb1f8-5be7-4bca-b701-0745cb37c5b6,1a763044-e08b-4e98-9419-7ba730028af1,6bf87306-ecc1-4c11-9d8f-f1cf723521e1,bebe3dcb-46b8-457e-8333-c9aa21389f19\n",
    "\n",
    "    # download zip to the current directory\n",
    "    # each \"code\" separated by commas in this URL basically represents a file \n",
    "    # concluded by trial and error on the ESCO site\n",
    "\n",
    "    # url = \"https://ec.europa.eu/esco/portal/downloadAction/9e75dade-397d-4fe4-ba3a-20cac74820f0,091f7d25-07c7-4a72-a673-d9b765d7db08,946edf49-3609-470c-8505-febf3dd72f84,09f931c8-2984-4e5e-b049-8e7c466bab04,215b7cb4-9818-4f0e-8d0d-7fdea1b83b9f,a9ccb1f8-5be7-4bca-b701-0745cb37c5b6,1a763044-e08b-4e98-9419-7ba730028af1,6bf87306-ecc1-4c11-9d8f-f1cf723521e1,bebe3dcb-46b8-457e-8333-c9aa21389f19\"\n",
    "    url = \"https://esco.ec.europa.eu/sites/default/files/tmp/ESCO%20dataset%20-%20v1.0.9%20-%20classification%20-%20en%20-%20csv.zip\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(\"esco-bundle.zip\", 'wb').write(r.content)\n",
    "\n",
    "    # extract files from zip bundle\n",
    "    with zipfile.ZipFile(\"esco-bundle.zip\",\"r\ as zip_ref:\n",
    "        zip_ref.extractall(path=folder)\n",
    "\n",
    "    # clean up\n",
    "    os.remove('esco-bundle.zip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_new_data:\n",
    "    print(\"Download and organize NICE data.\\n",
    "    folder = os.path.join(neo4j_import_folder, \"nice\\n",
    "\n",
    "    ### DOWNLOAD DATA & SEPARATE SHEETS IN THIS WORKBOOK INTO THEIR OWN DOCUMENTS ###\n",
    "    file_path = os.path.join(folder, \"nice_reference_spreadsheet.xlsx\\n",
    "    url = \"https://www.nist.gov/document/supplementnicespecialtyareasandworkroleksasandtasksxlsx\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(file_path, 'wb').write(r.content)\n",
    "    xlsx = pd.ExcelFile(file_path, engine='openpyxl')\n",
    "    for s in xlsx.sheet_names:\n",
    "        df = pd.read_excel(file_path, s, engine='openpyxl')\n",
    "        new_file_path = os.path.join(folder, s.replace('-', '_').replace(' ', '_').lower()+\".csv\\n",
    "        df.to_csv(new_file_path, index = False)\n",
    "\n",
    "    ### CREATE UNUSED FOLDER & MOVE INITIAL FILES ###\n",
    "    unused_folder = os.path.join(folder, \"unused\\n",
    "    if not os.path.exists(unused_folder):\n",
    "        os.mkdir(unused_folder)\n",
    "    # these are either not important to KSATT, empty, or a repeat of what's in the role-specific ksatt files\n",
    "    unused_files = [\"changes_from_draft_to_final\", \"cover_page\", \"errata\", \"specialty_areas_over_time\", \"master_ksa_list\", \"master_task_list\"]\n",
    "    for f in unused_files:\n",
    "        os.rename(os.path.join(folder, f\"{f}.csv\, os.path.join(unused_folder, f\"{f}.csv\)\n",
    "\n",
    "    ### FIX TABLE OF CONTENTS, RESAVE IN DIFFERENT FILE, AND MOVE TABLE OF CONTENTS TO UNUSED FOLDER ### \n",
    "    f = \"table_of_contents.csv\"\n",
    "    df = pd.read_csv(os.path.join(folder, f))\n",
    "    df.columns = df.iloc[1] # reassign headers to be actually correct\n",
    "    df = df.iloc[2:] # remove first two rows\n",
    "    df.drop([\"KSAs\", \"Tasks\"], axis=1, inplace=True) # drop a couple columns that don't matter (we have the code & these link to other excel sheets)\n",
    "    df[\"NICE Category\"] = df[\"NICE Specialty Area\"][pd.isna(df[\"Work Role ID\"])] # move the subheaders to another column\n",
    "    df[\"NICE Category\"].ffill(inplace=True) # forward fill the category column so there are no blanks there\n",
    "    df = df[df[\"Work Role ID\"].notna()] # drop the rows with subheaders only (aka ones with no work role IDs)\n",
    "    df.ffill(inplace=True) # forward fill the rest of the dataframe\n",
    "    df.dropna(axis=1, inplace=True) # drop empty columns\n",
    "    # separate out some column contents for KSATT prep\n",
    "    df[\"NICE Specialty Area Acronym\"] = df[\"NICE Specialty Area\"].str.extract(r\"\\(([A-Za-z]+)\\)\", expand=False)\n",
    "    df[\"NICE Specialty Area Title\"] = df.apply(lambda x: x[\"NICE Specialty Area\"].replace(\" (\"+x[\"NICE Specialty Area Acronym\"]+\\", \"\, axis=1)\n",
    "    df[\"NICE Category Title & Acronym\"] = df[\"NICE Category\"].str.split(\" - \.str[0]\n",
    "    df[\"NICE Category Description\"] = df[\"NICE Category\"].str.split(\" - \.str[1]\n",
    "    df[\"NICE Category Acronym\"] = df[\"NICE Category\"].str.extract(r\"\\(([A-Za-z]+)\\)\", expand=False)\n",
    "    df[\"NICE Category Title\"] = df.apply(lambda x: x[\"NICE Category Title & Acronym\"].replace(\" (\"+x[\"NICE Category Acronym\"]+\\", \"\, axis=1)\n",
    "    df.to_csv(os.path.join(folder, \"nice_areas_and_roles.csv\, index=False) # save as new file\n",
    "    os.rename(os.path.join(folder, f), os.path.join(unused_folder, f)) # move old file\n",
    "\n",
    "    ### FIX KSA & TASK FILES - COMBIEN THEM INTO A SINGLE FILE FOR EACH WORK ROLE ID ###\n",
    "    # first, create a folder for separate ksa and task files that we combine\n",
    "    separate_folder = os.path.join(folder, \"separate_ksa_task_files\\n",
    "    if not os.path.exists(separate_folder):\n",
    "        os.mkdir(separate_folder)\n",
    "    # now, combine ALL the separate ksa and task files into one big KSATT dataframe to save to a master file with Work Role ID mappings\n",
    "    ksatt_df = pd.DataFrame(columns=['Work Role ID', 'ID', 'Title'])\n",
    "    for filename in os.listdir(folder):\n",
    "        if 'ksas' in filename or 'tasks' in filename: # if you're dealing with a ksa or task file\n",
    "            df = pd.read_csv(os.path.join(folder, filename)) # read the df\n",
    "            os.rename(os.path.join(folder, filename), os.path.join(separate_folder, filename)) # move the separate ksa and task files to the separate folder\n",
    "            df = df.iloc[4:] # remove first several rows of df\n",
    "            df = df.loc[:, ~df.columns.str.contains('^Unnamed')] # remove any unnamed column junk \n",
    "            df.columns = ['ID', 'Title', ''] # reassign headers to be actually correct (and the same as what we'll merge with)\n",
    "            work_role_id = filename.replace(\"_ksas.csv\", \"\.replace(\"_tasks.csv\", \"\ # get the work role id from the file name\n",
    "            df[\"Work Role ID\"] = work_role_id.replace(\"_\", \"-\.upper() # add a column for Work Role ID (makes our lives easier when mapping later in KSATT)\n",
    "            ksatt_df = ksatt_df.append(df, ignore_index=True)\n",
    "    # do some finishing cleanup & save the dataframe to a file\n",
    "    ksatt_df = ksatt_df[ksatt_df['Title'].notna()] # only keep rows where Title isn't blank (don't need clarification on Knowledge vs Skills section)\n",
    "    ksatt_df.to_csv(os.path.join(folder, f\"master_ksatt_list_mapped.csv\, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosswalk file\n",
    "file_name = \"opm_onet_crosswalk.csv\"\n",
    "df = pd.read_csv(os.path.join(neo4j_import_folder, file_name))\n",
    "trusted_df = cleaner.main(df, os.path.join(neo4j_import_folder, f\"trusted_{file_name}\)\n",
    "\n",
    "# open source data files\n",
    "folders = [\"onet\", \"esco\", \"nice\"]\n",
    "for data_folder in folders:\n",
    "    for file_name in os.listdir(os.path.join(neo4j_import_folder, data_folder)):\n",
    "        if \".csv\" in file_name and \"trusted\" not in file_name:\n",
    "            print(f\"Cleaning {file_name}.\\n",
    "            df = pd.read_csv(os.path.join(neo4j_import_folder, data_folder, file_name))\n",
    "            trusted_df = cleaner.main(df, os.path.join(neo4j_import_folder, data_folder, f\"trusted_{file_name}\)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Neo4j"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear Database and Assert Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removing all nodes and clearing the database schema for a fresh start\\n",
    "# NOTE: had issue with broken connection or wire error, something like that - testing to see if upping heap size in neo4j.conf for the new database helps\n",
    "run_reg_query(\"\"\"MATCH (n) DETACH DELETE n\"\"\\n",
    "run_reg_query(\"\"\"CALL apoc.schema.assert({},{}, true)\"\"\ # clear schema\n",
    "\n",
    "print(\"Assert unique index properties for nodes related to employee data - this makes searching faster and organization neater\\n",
    "run_reg_query(\"\"\"CALL apoc.schema.assert(null, {\n",
    "    ONET_Occupation: ['key', 'title', 'description'],\n",
    "    ONET_Major_Group: ['key'],\n",
    "    ONET_Minor_Group: ['key'],\n",
    "    ONET_Broad_Occupation_Group: ['key'],\n",
    "    ONET_Detailed_Occupation_Group: ['key'],\n",
    "\n",
    "    ONET_Scale: ['key'],\n",
    "    \n",
    "    ONET_Segment:['key', 'title'], \n",
    "    ONET_Family:['key', 'title'], \n",
    "    ONET_Class:['key', 'title'], \n",
    "    ONET_Commodity:['key', 'title'],\n",
    "\n",
    "    ONET_Technology_Skills:['key', 'title'],\n",
    "    ONET_Tools:['key', 'title'],\n",
    "    ONET_Technology_Skills_Example:['title'],\n",
    "    ONET_Tools_Example:['title'],\n",
    "    \n",
    "    ONET_Tasks:['key', 'title'],\n",
    "\n",
    "    ONET_Abilities:['key', 'title', 'description'],\n",
    "    ONET_Interests_And_Work_Values:['key', 'title'],\n",
    "    ONET_Work_Styles:['key', 'title', 'description'],\n",
    "    ONET_Basic_Skills:['key', 'title', 'description'],\n",
    "    ONET_Cross_Functional_Skills:['key', 'title', 'description'],\n",
    "    ONET_Knowledge:['key', 'title', 'description'],\n",
    "    ONET_Education:['key', 'title', 'description'],\n",
    "    ONET_Education_Category:['key'],\n",
    "    ONET_Experience_And_Training:['key', 'title', 'description'],\n",
    "    ONET_Experience_And_Training_Category:['key'],\n",
    "    ONET_Generalized_Work_Activities:['key', 'title', 'description'],\n",
    "    ONET_Intermediate_Work_Activities:['key', 'title', 'description'],\n",
    "    ONET_Detailed_Work_Activities:['key', 'title', 'description'],\n",
    "    ONET_Work_Context:['key', 'title', 'description'],\n",
    "    \n",
    "    ESCO_Skills:['uri', 'key', 'alt_labels'],\n",
    "    ESCO_Knowledge:['uri', 'key', 'description', 'alt_labels'],\n",
    "    ESCO_Language:['uri', 'key', 'title', 'description', 'alt_labels'],\n",
    "    ESCO_Attitudes_Values:['uri', 'key', 'description', 'alt_labels'],\n",
    "    ESCO_Occupation_Group:['uri', 'key', 'alt_labels', 'description'],\n",
    "    ESCO_Occupation:['uri', 'title', 'alt_labels', 'description', 'key'],\n",
    "\n",
    "    NICE_Category:['title', 'acronym', 'description'],\n",
    "    NICE_Area:['title', 'acronym', 'description'],\n",
    "    NICE_Workrole:['key', 'title', 'description'],\n",
    "    OPM_Cybersecurity_Category:['key'],\n",
    "    NICE_Knowledge:['title'],\n",
    "    NICE_Skills:['title'],\n",
    "    NICE_Abilities:['title'],\n",
    "    NICE_Tasks:['title']\n",
    "    }, false);\n",
    "\"\"\"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload ONET Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Add Scale nodes for later reference\\n",
    "\n",
    "run_apoc_query(\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "        \"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_scalesreference.csv' AS row RETURN row\",\n",
    "        \"\n",
    "        MERGE (scale:ONET_Scale {key: row.scale_id})\n",
    "        ON CREATE SET scale.title = toLower(row.scale_name),\n",
    "        scale.min = toInteger(row.minimum),\n",
    "        scale.max = toInteger(row.maximum)\n",
    "        \",\n",
    "    {batchSize:1000, parallel:true, retries: 10}) YIELD operations;\n",
    "\"\"\ \n",
    "\n",
    "print(\"Add ONET Occupation nodes\\n",
    "# Use occupations to identify various jobs - most useful when combined with information about skills. \n",
    "# Each **Occupation** node has a description which provides additional vocabulary and a code that is an easy way to match in the ONET occupation hierarchy. \n",
    "run_apoc_query(\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "        \"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_occupationdata.csv' AS row RETURN row\",\n",
    "        \"\n",
    "        MERGE (a:ONET_Occupation {key: row.onet_soc_code, title: toLower(row.title)})\n",
    "        ON CREATE SET a.description = toLower(row.description)\n",
    "        \",\n",
    "    {batchSize:1000, parallel:true, retries: 10}) YIELD operations;\n",
    "\"\"\\n",
    "\n",
    "print(\"Add detailed ONET Occupation nodes - just add new unique occupations and relate to existing ones\\n",
    "# Added ONET_Occupation nodes are detailed occupations of existing ONET_Occupation nodes -- these relate to each other through a DETAIL_OF relationship.\n",
    "# Use: hierarchy of general and detailed occupations provides insight into how occupations might branch into focuses. Also, provides additional vocabulary.\n",
    "run_apoc_query(\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "        \"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_detailed.csv' AS row RETURN row\",\n",
    "        \"\n",
    "        WITH split(row.onet_soc_2019_code, '.') AS soc_id, row\n",
    "        MATCH (a:ONET_Occupation {key: soc_id[0]+'.00'})\n",
    "        MERGE (b:ONET_Occupation {key: soc_id[0]+'.'+soc_id[1]})\n",
    "        ON CREATE SET b.title = toLower(row.onet_soc_2019_title), b.description = toLower(row.onet_soc_2019_description)\n",
    "        WITH a, b\n",
    "        WHERE a.key <> b.key\n",
    "        MERGE (a)<-[:DETAIL_OF]-(b)\n",
    "        \",\n",
    "    {batchSize:1000, parallel:true, retries: 10}) YIELD operations;\n",
    "\"\"\\n",
    "# check\n",
    "results = run_reg_query(\"\"\"\n",
    "    MATCH p=(a:ONET_Occupation)<-[:DETAIL_OF]-(b:ONET_Occupation)\n",
    "    WITH split(a.key, '.') AS parent_id, split(b.key, '.') AS child_id, p\n",
    "    WHERE parent_id[1] = '00' AND child_id[1] <> '00' AND parent_id[0] = child_id[0]\n",
    "    RETURN count(p) AS num\n",
    "\"\"\['num'][0]\n",
    "if results == 0:\n",
    "    interrupt('Detailed ONET Occupations', \"detailed occupations should be the ones that have the more specific id\\n",
    "\n",
    "print(\"Add additional ONET occupations that are alternate titles for existing occupation nodes\\n",
    "# ONET_Occupation nodes connect through the ALT_TITLE_FOR relationship.\n",
    "# Use: help find occupations when having trouble matching the exact title + provide additional vocabulary\n",
    "run_apoc_query(\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "         \"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_alternatetitles.csv' AS row RETURN row\",\n",
    "        \"\n",
    "        MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "        MERGE (b:ONET_Occupation {title: toLower(row.alternate_title)})\n",
    "        WITH a, b, row\n",
    "        MERGE (a)<-[:ALT_TITLE_FOR]-(b)\n",
    "        \",\n",
    "    {batchSize:1000}) YIELD operations;\n",
    "\"\"\\n",
    "# check\n",
    "results = run_reg_query(\"\"\"MATCH n=(:ONET_Occupation)-[:ALT_TITLE_FOR]-(:ONET_Occupation) RETURN count(n) AS num\"\"\['num'][0]\n",
    "if results == 0:\n",
    "    interrupt('Alternate Titles ONET', 'there should be ALT_TITLE_FOR relationships and there are not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Create occupation groups\\n",
    "run_apoc_query(\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "        \"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_soc_structure.csv' AS row RETURN row\",\n",
    "        \"WHERE row.major_group<>'' MERGE (a:ONET_Major_Group {key: row.major_group})\",\n",
    "    {batchSize:1000, parallel:true, retries: 10}) YIELD operations;\n",
    "\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "        \"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_soc_structure.csv' AS row RETURN row\",\n",
    "        \"WHERE row.minor_group<>'' MERGE (a:ONET_Minor_Group {key: row.minor_group})\",\n",
    "    {batchSize:1000, parallel:true, retries: 10}) YIELD operations;\n",
    "\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "        \"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_soc_structure.csv' AS row RETURN row\",\n",
    "        \"WHERE row.broad_occupation<>'' MERGE (a:ONET_Broad_Occupation_Group {key: row.broad_occupation})\",\n",
    "    {batchSize:1000, parallel:true, retries: 10}) YIELD operations;\n",
    "\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "        \"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_soc_structure.csv' AS row RETURN row\",\n",
    "        \"WHERE row.detailed_occupation<>'' MERGE (a:ONET_Detailed_Occupation_Group {key: row.detailed_occupation})\",\n",
    "    {batchSize:1000, parallel:true, retries: 10}) YIELD operations;\n",
    "\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "        \"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_soc_structure.csv' AS row RETURN row\",\n",
    "        \"\n",
    "        MATCH (a:ONET_Major_Group {key: row.major_group})\n",
    "        MATCH (b:ONET_Minor_Group {key: row.minor_group})\n",
    "        MATCH (c:ONET_Broad_Occupation_Group {key: row.broad_occupation})\n",
    "        MATCH (d:ONET_Detailed_Occupation_Group {key: row.detailed_occupation})\n",
    "        MATCH (e:ONET_Occupation {title: toLower(row.soc_or_onet_soc_2019_title)})\n",
    "\n",
    "        MERGE (a)<-[:IN_GROUP]-(b)\n",
    "        MERGE (b)<-[:IN_GROUP]-(c)\n",
    "        MERGE (c)<-[:IN_GROUP]-(d)\n",
    "        MERGE (d)<-[:IN_GROUP]-(e)\n",
    "        \",\n",
    "    {batchSize:1000, parallel:true, retries: 10}) YIELD operations;\n",
    "\"\"\\n",
    "\n",
    "results = run_reg_query(\"\"\"\n",
    "MATCH p=(:ONET_Major_Group)<-[:IN_GROUP]-(:ONET_Minor_Group)<-[:IN_GROUP]-(:ONET_Broad_Occupation_Group)<-[:IN_GROUP]-(:ONET_Detailed_Occupation_Group)<-[:IN_GROUP]-(:ONET_Occupation) \n",
    "RETURN count(p) AS num\n",
    "\"\"\['num'][0]\n",
    "if results <= 1:\n",
    "    interrupt(\"Occupation Groups\", \"there should be relationships creating the occupation group hierarchy\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Load ONET elements (aka KSATTs) and establish a hierarchy based on keys.\\n",
    "# Use: hierarchy of KSATTs\n",
    "# Hierarchy established through SUB_ELEMENT_OF relationship to other **ONET_Element** nodes - check out with:\n",
    "# MATCH p=(:ONET_Element {key: '0'})<-[:SUB_ELEMENT_OF]-(b)<-[:SUB_ELEMENT_OF]-(c)<-[:SUB_ELEMENT_OF]-(d) return p limit 50\n",
    "# The \"leaf\" nodes — e.g. **ONET_Element** nodes that are the most detailed in the hierarchy — are re-labeled with their appropriate category.\n",
    "# load the elements with key, title and description properties \n",
    "run_apoc_query(\"\"\"\n",
    "    CALL apoc.periodic.iterate(\n",
    "        \"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_contentmodelreference.csv' AS row RETURN row\",\n",
    "        \"\n",
    "        MERGE (a:ONET_Element {key: row.element_id})\n",
    "        ON CREATE SET a.title = toLower(row.element_name), a.description = toLower(row.description)\n",
    "        \",\n",
    "    {batchSize:1000, parallel:true, retries: 10}) YIELD operations;\n",
    "\"\"\\n",
    "# relate elements with the SUB_ELEMENT_OF relationship that have key's like 1.A.1 and 1.A.1.a\n",
    "run_reg_query(\"\"\"\n",
    "    MATCH (a:ONET_Element), (b:ONET_Element)\n",
    "    WHERE b.key CONTAINS a.key AND a.key <> b.key AND size(b.key)-2 = size(a.key)\n",
    "    MERGE (a)<-[:SUB_ELEMENT_OF]-(b)\n",
    "\"\"\\n",
    "# rename lower-level nodes if they match another element node with a shorter key\n",
    "run_reg_query(\"\"\"\n",
    "    MATCH (a:ONET_Element), (b:ONET_Element)\n",
    "    WHERE a.title = b.title AND size(a.key) < size(b.key)\n",
    "    SET a.title = a.title+' category'\n",
    "\"\"\\n",
    "# relate main ONET_Element nodes (with keys 1-6) to the new central Element node (key 0)\n",
    "for i in range(1,7):\n",
    "    run_reg_query(\"\"\"\n",
    "        MATCH (a:ONET_Element {{key: '{}'}}) \n",
    "        MERGE (b:ONET_Element {{key: '0', title: 'Element'}}) \n",
    "        MERGE (b)<-[:SUB_ELEMENT_OF]-(a);\n",
    "    \"\"\".format(i))\n",
    "\n",
    "# get title of ONET_Element node\n",
    "# assign label of those sub elements as that title, but only go down so far (to Abilities, Basic Skills, etc. level)\n",
    "titles = [ t for t in run_reg_query(\"\"\"MATCH (:ONET_Element {key: '0'})<-[:SUB_ELEMENT_OF]-(a:ONET_Element) RETURN a.title AS title\"\"\['title']]\n",
    "for title in titles:\n",
    "    num_sub = run_reg_query(\"\"\"MATCH (a {{title: \"{}\"}})<-[:SUB_ELEMENT_OF]-(b:ONET_Element) WHERE size(b.key) < 6 RETURN count(b) AS num\"\"\".format(title))['num'][0]\n",
    "    if num_sub > 0:\n",
    "        label = title.title().replace(' ', '_').replace('-', '_').replace(',', '_').replace(':', '_')\n",
    "        new_titles = [ t for t in run_reg_query(\"\"\"\n",
    "            MATCH (a {{title: \"{}\"}})<-[:SUB_ELEMENT_OF]-(b:ONET_Element)\n",
    "            WHERE size(b.key) < 6\n",
    "            SET b:ONET_{} REMOVE b:ONET_Element\n",
    "            RETURN b.title AS title\n",
    "        \"\"\".format(title, label))['title']]\n",
    "        for nt in new_titles:\n",
    "            titles.append(nt)\n",
    "\n",
    "# picking up at the nodes we stopped at, assign specific labels further down\n",
    "num_left = run_reg_query(\"\"\"MATCH (a)<-[:SUB_ELEMENT_OF]-(n:ONET_Element) WHERE a.key <> '0' RETURN count(n) AS num\"\"\['num'][0]\n",
    "while num_left > 0:\n",
    "    for title in titles:\n",
    "        label = title.title().replace(' ', '_').replace('-', '_').replace(',', '_').replace(':', '_')\n",
    "        print(label)\n",
    "        run_reg_query(\"\"\"MATCH (:ONET_{})<-[:SUB_ELEMENT_OF]-(b:ONET_Element) SET b:ONET_{} REMOVE b:ONET_Element\"\"\".format(label, label))\n",
    "    num_left = run_reg_query(\"\"\"MATCH (a)<-[:SUB_ELEMENT_OF]-(n:ONET_Element) WHERE a.key <> '0' RETURN count(n) AS num\"\"\['num'][0]\n",
    "    print(num_left)\n",
    "    \n",
    "\n",
    "# NOTE\n",
    "# we don't really end up using\n",
    "# Experience and Training\n",
    "# Basic Skills Entry Requirement\n",
    "# Cross Functional Skills Entry Requirement\n",
    "# Licensing\n",
    "# Organizational Context \n",
    "# Titles\n",
    "# Description\n",
    "# Alternate Titles\n",
    "# Labor Market Information\n",
    "# Occupational Outlook\n",
    "\n",
    "results = run_reg_query(\"\"\"MATCH p=(:ONET_Element {key: '0'})<-[:SUB_ELEMENT_OF]-(:ONET_Element {key: '1'}) RETURN count(p) AS num\"\"\['num'][0]\n",
    "if results != 1:\n",
    "    interrupt(\"ONET_Element relationships\", \"there should be relationships between element 0 and 1..6\\n",
    "\n",
    "results = run_reg_query(\"\"\"MATCH p=(a {key: '1.A.1'})<-[:SUB_ELEMENT_OF]-(b {key: '1.A.1.a'}) RETURN count(p) AS num\"\"\['num'][0]\n",
    "if results != 1:\n",
    "    interrupt(\"ONET_Element relationships\", \"there should be relationships between elements with 'sub-codes'\\n",
    "\n",
    "results = run_reg_query(\"\"\"MATCH p=(:ONET_Element)<-[:SUB_ELEMENT_OF]-(:ONET_Worker_Characteristics)<-[:SUB_ELEMENT_OF]-(:ONET_Abilities)<-[:SUB_ELEMENT_OF]-(:ONET_Abilities) RETURN count(p) AS num\"\"\['num'][0]\n",
    "if results < 1:\n",
    "    interrupt(\"ONET_Element relationships\", \"there should be relationships between elements in a chain\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Delineate which Commodity nodes are Technology Skills and Tools\\n",
    "# Use: specify the technologies and tools in the database for reference of specific skill types.\n",
    "# Some **ONET_Commodity** nodes are in fact technologies or tools. So after the **ONET_Commodity** nodes are created, we additionally label those **ONET_Commodity** nodes as **ONET_Technology_Skills** and **ONET_Tools** nodes — _so these have multiple labels_ - depending on which are in technologyskills.csv and toolsused.csv. \n",
    "# **ONET_Technology_Skills** nodes have a trending_technology property to indicate if ONET determines it is a rising skill or not\n",
    "# **ONET_Technology_Skills** and **ONET_Tools** are related to their parent **ONET_Occupation_Specific_Information** nodes. \n",
    "# We do not remove the **ONET_Commodity**'s previously created relationship to **ONET_Class**.\n",
    "\n",
    "# technology skills\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_technologyskills.csv' AS row RETURN row\",\n",
    "\"\n",
    "MERGE (a:ONET_Technology_Skills {key: row.commodity_code})\n",
    "ON MATCH SET a:ONET_Commodity\n",
    "ON CREATE SET a:ONET_Commodity, a.title = toLower(row.commodity_title), a.trending_technology = row.hot_technology\n",
    "MERGE (b:ONET_Technology_Skills_Example {title: toLower(row.example)})\n",
    "ON CREATE SET b.trending_technology = row.hot_technology\n",
    "MERGE (a)<-[:EXAMPLE_OF]-(b)\n",
    "\",{batchSize:5000}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_technologyskills.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation_Specific_Information {key:'5.F'}), (b:ONET_Technology_Skills {key: row.commodity_code})\n",
    "MERGE (a)<-[:SUB_ELEMENT_OF]-(b)\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "# tools\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_toolsused.csv' AS row RETURN row\",\n",
    "\"\n",
    "MERGE (a:ONET_Tools {key: row.commodity_code})\n",
    "ON MATCH SET a:ONET_Commodity\n",
    "ON CREATE SET a:ONET_Commodity, a.title = toLower(row.commodity_title)\n",
    "MERGE (b:ONET_Tools_Example {title: toLower(row.example)})\n",
    "MERGE (a)<-[:EXAMPLE_OF]-(b)\n",
    "\",{batchSize:5000}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_toolsused.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation_Specific_Information {key:'5.G'}), (b:ONET_Tools {key: row.commodity_code})\n",
    "MERGE (a)<-[:SUB_ELEMENT_OF]-(b)\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Create a hierarchical pattern of Segment, Family, Class and Commodity nodes\\n",
    "\n",
    "# Hierarchy created from Segment <- Family <- Class <- Commodity\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_unspscreference.csv' AS row RETURN row\",\n",
    "\"\n",
    "MERGE (a:ONET_Segment {key: row.segment_code})\n",
    "SET a.title = toLower(row.segment_title)\n",
    "MERGE (b:ONET_Family {key: row.family_code})\n",
    "SET b.title = toLower(row.family_title)\n",
    "MERGE (c:ONET_Class {key: row.class_code})\n",
    "SET c.title = toLower(row.class_title)\n",
    "MERGE (d:ONET_Commodity {key: row.commodity_code})\n",
    "MERGE (a)<-[:SUB_SEGMENT_OF]-(b)\n",
    "MERGE (b)<-[:SUB_SEGMENT_OF]-(c)\n",
    "MERGE (c)<-[:SUB_SEGMENT_OF]-(d)\n",
    "\",{batchSize:5000}) YIELD operations;\"\"\\n",
    "\n",
    "print(\"Check that the Commodity hierachy and Technology Skills/Tools queries completed as expected\\n",
    "check_queries = []\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Technology_Skills)--(:ONET_Technology_Skills_Example) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Tools)--(:ONET_Tools_Example) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH (a) WHERE a:ONET_Commodity AND a:ONET_Technology_Skills RETURN count(a) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH (a) WHERE a:ONET_Commodity AND a:ONET_Tools RETURN count(a) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Segment)<-[:SUB_SEGMENT_OF]-(:ONET_Family)<-[:SUB_SEGMENT_OF]-(:ONET_Class)<-[:SUB_SEGMENT_OF]-(:ONET_Commodity) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Segment)<-[:SUB_SEGMENT_OF]-(:ONET_Family)<-[:SUB_SEGMENT_OF]-(:ONET_Class)<-[:SUB_SEGMENT_OF]-(:ONET_Technology_Skills) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Segment)<-[:SUB_SEGMENT_OF]-(:ONET_Family)<-[:SUB_SEGMENT_OF]-(:ONET_Class)<-[:SUB_SEGMENT_OF]-(:ONET_Tools) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(a:ONET_Occupation_Specific_Information {key:'5.F'})<-[:SUB_ELEMENT_OF]-(b:ONET_Technology_Skills) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(a:ONET_Occupation_Specific_Information {key:'5.G'})<-[:SUB_ELEMENT_OF]-(b:ONET_Tools) RETURN count(p) AS num\"\"\\n",
    "# these should really be more than 1\n",
    "for cq in check_queries:\n",
    "    results = run_reg_query(cq)['num'][0]\n",
    "    if results <= 1:\n",
    "        interrupt(\"Segment/Family/Class/Commodity/Tech Skills/Tools\", \"there should be more nodes and relationships\\n",
    "\n",
    "num_tech = run_reg_query(\"\"\"MATCH (a:ONET_Technology_Skills) RETURN count(a) AS num\"\"\['num'][0]\n",
    "num_tools = run_reg_query(\"\"\"MATCH (a:ONET_Tools) RETURN count(a) AS num\"\"\['num'][0]\n",
    "num_comm = run_reg_query(\"\"\"MATCH (a:ONET_Commodity) RETURN count(a) AS num\"\"\['num'][0]\n",
    "if num_comm != (num_tech+num_tools-2):\n",
    "    interrupt(\"Number of Commodity vs. Tech Skills & Tools\", \"there should be an even number of Commodities and Tech Skills + Tools (-2 because of ONET taxonomy import)\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Create a hierarchy of work activities (generalized, intermediate, detailed)\\n",
    "# Use: be able to look at work activities at different levels of granularity\n",
    "# **ONET_Generalized_Work_Activities** were already created from the contentmodelreference.csv (all **ONET_Element** nodes that started as 4.A were re-labeled as **ONET_Generalized_Work_Activities**)\n",
    "# Intermediate & Detailed were not detailed in contentmodelreference.csv but are in this document, so we're creating these\n",
    "# **ONET_Generalized_Work_Activities** <- **ONET_Intermediate_Work_Activities** <- **ONET_Detailed_Work_Activities**\n",
    "# **ONET_Intermediate_Work_Activities** and **ONET_Detailed_Work_Activities** are related to their parents in the element hierarchy\n",
    "\n",
    "# Generalized_Work_Activities were already created from contentmodelreference.csv, so we need to match those\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_dwareference.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Generalized_Work_Activities {key: row.element_id})\n",
    "MATCH (bb:ONET_Occupational_Requirements {title:'intermediate work activities'})\n",
    "MATCH (cc:ONET_Occupational_Requirements {title:'detailed work activities'})\n",
    "MERGE (b:ONET_Intermediate_Work_Activities {key: row.iwa_id, title: toLower(row.iwa_title)})\n",
    "MERGE (c:ONET_Detailed_Work_Activities {key: row.dwa_id, title: toLower(row.dwa_title)})\n",
    "\n",
    "MERGE (a)<-[:DETAIL_OF]-(b)\n",
    "MERGE (b)<-[:DETAIL_OF]-(c)\n",
    "MERGE (bb)<-[:SUB_ELEMENT_OF]-(b)\n",
    "MERGE (cc)<-[:SUB_ELEMENT_OF]-(c)\n",
    "\n",
    "\",{batchSize:1000}) YIELD operations;\"\"\\n",
    "\n",
    "check_queries = []\n",
    "\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupational_Requirements)<-[:SUB_ELEMENT_OF]-(:ONET_Generalized_Work_Activities) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupational_Requirements)<-[:SUB_ELEMENT_OF]-(:ONET_Intermediate_Work_Activities) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupational_Requirements)<-[:SUB_ELEMENT_OF]-(:ONET_Detailed_Work_Activities) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Generalized_Work_Activities)<-[:DETAIL_OF]-(:ONET_Intermediate_Work_Activities)<-[:DETAIL_OF]-(:ONET_Detailed_Work_Activities) RETURN count(p) AS num\"\"\\n",
    "\n",
    "for cq in check_queries:\n",
    "    results = run_reg_query(cq)['num'][0]\n",
    "    if results < 1:\n",
    "        interrupt(\"Work Activities\", \"there should be chains of work activities\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Create task nodes\\n",
    "# Use: different tasks performed as part of occupations or work activities imply skills, which we are interested in\n",
    "# These were not included in contentmodelreference.csv, only the parent Tasks **ONET_Element** node was, so this loads detailed **ONET_Tasks** nodes\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_taskstatements.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation_Specific_Information {title: 'tasks'})\n",
    "MERGE (b:ONET_Tasks {title: toLower(row.task)})\n",
    "ON CREATE SET b.key = row.task_id, b.description = toLower(row.task), b.type = toLower(row.task_type)\n",
    "ON MATCH SET b.key = row.task_id+', '+b.key\n",
    "MERGE (a)<-[:SUB_ELEMENT_OF]-(b)\n",
    "\",{batchSize:10000, parallel:false, retries: 10}) YIELD operations;\"\"\\n",
    "results = run_reg_query(\"\"\"MATCH p=(:ONET_Occupation_Specific_Information {title: 'tasks'})<-[:SUB_ELEMENT_OF]-(:ONET_Tasks) RETURN count(p) AS num\"\"\['num'][0]\n",
    "if results <= 1:\n",
    "    interrupt(\"Tasks\", \"there should be more relationships\\n",
    "\n",
    "print(\"Match tasks to detailed work activities\\n",
    "# Because **ONET_Tasks** are required in order to complete work activities, we want to associate **ONET_Detailed_Work_Activities** with requisite **ONET_Tasks**.\n",
    "# We don't create any nodes, we just MATCH existing **ONET_Tasks** and **ONET_Detailed_Work_Activities** nodes.\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_taskstodwas.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Detailed_Work_Activities {key: row.dwa_id})\n",
    "MATCH (b:ONET_Tasks {title: toLower(row.task)})\n",
    "WITH a, b, row\n",
    "MERGE (a)<-[:TASK_FOR_DWA]-(b)\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "results = run_reg_query(\"\"\"MATCH p=(:ONET_Detailed_Work_Activities)<-[:TASK_FOR_DWA]-(:ONET_Tasks) RETURN count(p) AS num\"\"\['num'][0]\n",
    "if results <= 1:\n",
    "    interrupt('Tasks & DWAs', 'there should be relationships between DWAs and Tasks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enrich the existing education and experiences and training data by adding categories\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_educationtrainingandexperiencecategories.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Education {key: row.element_id})\n",
    "MERGE (b:ONET_Education_Category {key: row.scale_id+'.'+row.category, title: toLower(row.category_description)})\n",
    "MERGE (a)<-[:CATEGORY_OF]-(b)\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_educationtrainingandexperiencecategories.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Experience_And_Training {key: row.element_id})\n",
    "MERGE (b:ONET_Experience_And_Training_Category {key: row.scale_id+'.'+row.category, title: toLower(row.category_description)})\n",
    "MERGE (a)<-[:CATEGORY_OF]-(b)\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "\n",
    "results = run_reg_query(\"\"\"MATCH p=(:ONET_Education)<-[:CATEGORY_OF]-(:ONET_Education_Category) RETURN count(p) AS num\"\"\['num'][0]\n",
    "if results <= 1:\n",
    "    interrupt(\"Education Categories\", \"there should be relationships between Education and categories\\n",
    "results = run_reg_query(\"\"\"MATCH p=(:ONET_Experience_And_Training)<-[:CATEGORY_OF]-(:ONET_Experience_And_Training_Category) RETURN count(p) AS num\"\"\['num'][0]\n",
    "if results <= 1:\n",
    "    interrupt(\"Experience and Training Categories\", \"there should be relationships between Experience and Training and categories\\n",
    "\n",
    "results = run_reg_query(\"\"\"MATCH p=(:ONET_Education)<-[:CATEGORY_OF]-(:ONET_Education_Category)-[:CATEGORY_OF]->(:ONET_Education) RETURN count(p) AS num\"\"\['num'][0]\n",
    "if results != 0:\n",
    "    interrupt(\"Education Categories\", \"there should not be relationships between an Education category and multiple Education nodes\\n",
    "results = run_reg_query(\"\"\"MATCH p=(:ONET_Experience_And_Training)<-[:CATEGORY_OF]-(:ONET_Experience_And_Training_Category)-[:CATEGORY_OF]->(:ONET_Experience_And_Training) RETURN count(p) AS num\"\"\['num'][0]\n",
    "if results != 0:\n",
    "    interrupt(\"Experience and Training Categories\", \"there should not be relationships between an Experience and Training category and multiple Experience and Training nodes\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Create relationships between occupations and the various KSATTs derived from ONET\\n",
    "# 1.  All KSATT nodes were originally **ONET_Element** nodes, they were just re-labeled. So they all have an id and title, and _SUB_ELEMENT_OF_ relationship. These are not being created, only matched.\n",
    "# 2.  Now they all will have relationships with **ONET_Occupation** nodes - these are _FOUND_IN_ relationships at the core, but are named differently based on the type of scale that the relationship indicates (e.g. _LV_FOUND_IN_ for level, _IM_FOUND_IN_ for importance, etc.). These will be cleaned up later.\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_abilities.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Abilities {key: row.element_id})\n",
    "WITH a, b, row\n",
    "CALL apoc.merge.relationship(b, toString(row.scale_id)+'_FOUND_IN', {data_value: toFloat(row.data_value)}, {}, a, {}) YIELD rel RETURN rel\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_interests.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Interests_And_Work_Values {key: row.element_id})\n",
    "WITH a, b, row\n",
    "CALL apoc.merge.relationship(b, toString(row.scale_id)+'_FOUND_IN', {data_value: toFloat(row.data_value)}, {}, a, {}) YIELD rel RETURN rel\n",
    "\",{batchSize:1000}) YIELD operations;\"\"\\n",
    "\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_workstyles.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Work_Styles {key: row.element_id})\n",
    "WITH a, b, row\n",
    "CALL apoc.merge.relationship(b, toString(row.scale_id)+'_FOUND_IN', {data_value: toFloat(row.data_value)}, {}, a, {}) YIELD rel RETURN rel\n",
    "\",{batchSize:1000}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_skills.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Basic_Skills {key: row.element_id})\n",
    "WITH a, b, row\n",
    "CALL apoc.merge.relationship(b, toString(row.scale_id)+'_FOUND_IN', {data_value: toFloat(row.data_value)}, {}, a, {}) YIELD rel RETURN rel\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_skills.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Cross_Functional_Skills {key: row.element_id})\n",
    "WITH a, b, row\n",
    "CALL apoc.merge.relationship(b, toString(row.scale_id)+'_FOUND_IN', {data_value: toFloat(row.data_value)}, {}, a, {}) YIELD rel RETURN rel\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_knowledge.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Knowledge {key: row.element_id})\n",
    "WITH a, b, row\n",
    "CALL apoc.merge.relationship(b, toString(row.scale_id)+'_FOUND_IN', {data_value: toFloat(row.data_value)}, {}, a, {}) YIELD rel RETURN rel\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_educationtrainingandexperience.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Education_Category {key: row.scale_id+'.'+row.category})\n",
    "WITH a, b, row\n",
    "CALL apoc.merge.relationship(b, toString(row.scale_id)+'_FOUND_IN', {data_value: toFloat(row.data_value)}, {}, a, {}) YIELD rel RETURN rel\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_educationtrainingandexperience.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Experience_And_Training_Category {key: row.scale_id+'.'+row.category})\n",
    "WITH a, b, row\n",
    "CALL apoc.merge.relationship(b, toString(row.scale_id)+'_FOUND_IN', {data_value: toFloat(row.data_value)}, {}, a, {}) YIELD rel RETURN rel\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "# here the GWAs are being matched but really we associated the DWA with the Occupations\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_workactivities.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Generalized_Work_Activities {key: row.element_id})\n",
    "WITH a, b, row\n",
    "CALL apoc.merge.relationship(b, toString(row.scale_id)+'_FOUND_IN', {data_value: toFloat(row.data_value)}, {}, a, {}) YIELD rel RETURN rel\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_workcontext.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Work_Context {key: row.element_id})\n",
    "WITH a, b, row\n",
    "CALL apoc.merge.relationship(b, toString(row.scale_id)+'_FOUND_IN', {data_value: toFloat(row.data_value)}, {}, a, {}) YIELD rel RETURN rel\n",
    "\",{batchSize:1000}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_taskratings.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Tasks {key: row.task_id})\n",
    "WITH a, b, row\n",
    "CALL apoc.merge.relationship(b, toString(row.scale_id)+'_FOUND_IN', {data_value: toFloat(row.data_value)}, {}, a, {}) YIELD rel RETURN rel\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "# TODO: no weights \n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_technologyskills.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Technology_Skills {key: row.commodity_code})\n",
    "WITH a, b, row\n",
    "MERGE (b)-[:FOUND_IN]->(a)\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "# TODO: no weights \n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///onet/trusted_toolsused.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation {key: row.onet_soc_code})\n",
    "MATCH (b:ONET_Tools {key: row.commodity_code})\n",
    "WITH a, b, row\n",
    "MERGE (b)-[:FOUND_IN]->(a)\n",
    "\",{batchSize:10000}) YIELD operations;\"\"\\n",
    "\n",
    "check_queries = []\n",
    "\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Abilities) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Interests_And_Work_Values) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Work_Styles) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Basic_Skills) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Cross_Functional_Skills) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Education_Category) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Experience_And_Training_Category) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Generalized_Work_Activities) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Work_Context) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Tasks) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Technology_Skills) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<--(:ONET_Tools) RETURN count(p) AS num\"\"\\n",
    "\n",
    "for cq in check_queries:\n",
    "    results = run_reg_query(cq)['num'][0]\n",
    "    if results <= 0:\n",
    "        interrupt(cq, \"there should be relationships\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clean FOUND_IN relationships\\n",
    "# gather all the different types of FOUND_IN relationships\n",
    "relationships = [x['rel'] for x in graph.run(\"\"\"MATCH (a)-[r]-(b) WHERE type(r) CONTAINS 'FOUND_IN' RETURN DISTINCT type(r) AS rel\"\"\.data()]\n",
    "# convert all the different types of FOUND_IN relationships to a FOUND_IN relationship with a normalized property of it's type instead\n",
    "# e.g. IM Scale from 1 to 10 | IM_FOUND_IN {data_value: 7} -> FOUND_IN {IM: 0.7}\n",
    "for r in relationships:\n",
    "    if r != 'FOUND_IN':\n",
    "        prefix = r.replace('_FOUND_IN', '')\n",
    "        print(prefix)\n",
    "        graph.run(\"\"\"\n",
    "            CALL {{MATCH (a:ONET_Scale {{key: '{}'}}) RETURN a.max AS max LIMIT 1}}\n",
    "            WITH max\n",
    "            MATCH (a)<-[r1:{}_FOUND_IN]-(b) MERGE (a)<-[r2:FOUND_IN]-(b) SET r2.{} = toFloat(r1.data_value)/max\n",
    "        \"\"\".format(prefix, prefix, prefix))\n",
    "        graph.run(\"\"\"MATCH (a)<-[r1:{}_FOUND_IN]-(b) DELETE r1\"\"\".format(prefix))\n",
    "check_queries = []\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Abilities) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Interests_And_Work_Values) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Work_Styles) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Basic_Skills) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Cross_Functional_Skills) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Education_Category) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Experience_And_Training_Category) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Generalized_Work_Activities) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Work_Context) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Tasks) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Technology_Skills) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ONET_Occupation)<-[:FOUND_IN]-(:ONET_Tools) RETURN count(p) AS num\"\"\\n",
    "for cq in check_queries:\n",
    "    results = run_reg_query(cq)['num'][0]\n",
    "    if results <= 0:\n",
    "        interrupt(\"Relationships between ONET Occupations and KSATTs\", \"there should be relationships\\n",
    "results = [item['rel'] for item in graph.run(\"\"\"MATCH (a)-[r]-(b) WHERE type(r) CONTAINS 'FOUND_IN' RETURN DISTINCT type(r) AS rel\"\"\.data()]\n",
    "print(results)\n",
    "if len(results) > 1 or 'FOUND_IN' not in results:\n",
    "    interrupt(\"FOUND_IN relationship cleaning\", \"only FOUND_IN relationship left should be not prefixxed by anything\"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload ESCO Data\n",
    "Notes:\n",
    "concept_Schemes didn't have anything helpful.  \n",
    "Everything from ictSkills was in transversalSkillsCollection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Add skill, knowledge, language, and attitudes and value hierarchy\\n",
    "\n",
    "# add skill hierarchy \n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_skillshierarchy_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "WHERE row.level_3_code <> '' AND row.level_0_preferred_term = 'skills'\n",
    "MERGE (a:ESCO_Skills {uri: row.level_1_uri})\n",
    "ON CREATE SET a.title = row.level_1_preferred_term\n",
    "MERGE (b:ESCO_Skills {uri: row.level_2_uri})\n",
    "ON CREATE SET b.title = row.level_2_preferred_term\n",
    "MERGE (c:ESCO_Skills {uri: row.level_3_uri})\n",
    "ON CREATE SET c.title = row.level_3_preferred_term, c.description = row.description\n",
    "\n",
    "MERGE (a)<-[:SUB_ELEMENT_OF]-(b)\n",
    "MERGE (b)<-[:SUB_ELEMENT_OF]-(c)\n",
    "\",\n",
    "{batchSize:1000, parallel:true, retries: 10}) YIELD operations;\"\"\\n",
    "\n",
    "# add knowledge hierarchy\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_skillshierarchy_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "WHERE row.level_3_preferred_term <> '' AND row.level_0_preferred_term = 'knowledge'\n",
    "MERGE (a:ESCO_Knowledge {uri: row.level_1_uri})\n",
    "ON CREATE SET a.title = row.level_1_preferred_term\n",
    "MERGE (b:ESCO_Knowledge {uri: row.level_2_uri})\n",
    "ON CREATE SET b.title = row.level_2_preferred_term\n",
    "MERGE (c:ESCO_Knowledge {uri: row.level_3_uri})\n",
    "ON CREATE SET c.title = row.level_3_preferred_term\n",
    "\n",
    "MERGE (a)<-[:SUB_ELEMENT_OF]-(b)\n",
    "MERGE (b)<-[:SUB_ELEMENT_OF]-(c)\n",
    "\n",
    "WITH c, row\n",
    "WHERE row.description <> ''\n",
    "SET c.description = row.description\n",
    "\",\n",
    "{batchSize:1000, parallel:true, retries: 10}) YIELD operations;\"\"\\n",
    "\n",
    "# add language hierarchy\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_skillshierarchy_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "WHERE row.level_1_code <> '' AND row.level_0_preferred_term = 'language skills and knowledge'\n",
    "MERGE (a:ESCO_Language {uri: row.level_1_uri})\n",
    "ON CREATE SET a.title = row.level_1_preferred_term, a.description = row.description\n",
    "\",\n",
    "{batchSize:1000, parallel:true, retries: 10}) YIELD operations;\"\"\\n",
    "\n",
    "# add attitudes and values hierarchy \n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_skillshierarchy_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "WHERE row.level_3_code <> '' AND row.level_0_preferred_term = 'attitudes and values'\n",
    "MERGE (a:ESCO_Attitudes_Values {uri: row.level_1_uri})\n",
    "ON CREATE SET a.title = row.level_1_preferred_term\n",
    "MERGE (b:ESCO_Attitudes_Values {uri: row.level_2_uri})\n",
    "ON CREATE SET b.title = row.level_2_preferred_term\n",
    "MERGE (c:ESCO_Attitudes_Values {uri: row.level_3_uri})\n",
    "ON CREATE SET c.title = row.level_3_preferred_term, c.description = row.description\n",
    "\n",
    "MERGE (a)<-[:SUB_ELEMENT_OF]-(b)\n",
    "MERGE (b)<-[:SUB_ELEMENT_OF]-(c)\n",
    "\",\n",
    "{batchSize:1000, parallel:true, retries: 10}) YIELD operations;\"\"\\n",
    "\n",
    "check_queries = []\n",
    "check_queries.append(\"\"\"MATCH p=(:ESCO_Skills)<-[:SUB_ELEMENT_OF]-(:ESCO_Skills)<-[:SUB_ELEMENT_OF]-(:ESCO_Skills) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ESCO_Knowledge)<-[:SUB_ELEMENT_OF]-(:ESCO_Knowledge)<-[:SUB_ELEMENT_OF]-(:ESCO_Knowledge) RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH (a:ESCO_Language) RETURN count(a) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(:ESCO_Attitudes_Values)<-[:SUB_ELEMENT_OF]-(:ESCO_Attitudes_Values)<-[:SUB_ELEMENT_OF]-(:ESCO_Attitudes_Values) RETURN count(p) AS num\"\"\\n",
    "for cq in check_queries:\n",
    "    results = run_reg_query(cq)['num'][0]\n",
    "    if results <= 1:\n",
    "        interrupt(\"ESCO KSATTs\", \"there should be more ESCO KSATTs or relationships between them\\n",
    "\n",
    "num_skills = run_reg_query(\"\"\"MATCH (a:ESCO_Skills) RETURN count(a) AS num\"\"\['num'][0]\n",
    "num_knowledge = run_reg_query(\"\"\"MATCH (a:ESCO_Knowledge) RETURN count(a) AS num\"\"\['num'][0]\n",
    "num_language = run_reg_query(\"\"\"MATCH (a:ESCO_Language) RETURN count(a) AS num\"\"\['num'][0]\n",
    "num_av = run_reg_query(\"\"\"MATCH (a:ESCO_Attitudes_Values) RETURN count(a) AS num\"\"\['num'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Add additional skills to the hierarchy\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_transversalskillscollection_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "WHERE row.skillType = 'skill/competence' \n",
    "WITH split(row.broaderconceptpt, ' | ') AS broaderconcepttitles, split(row.broaderconcepturi, ' | ') AS broaderconcepturis, row\n",
    "MERGE (a:ESCO_Skills {uri: broaderconcepturis[0]})\n",
    "ON CREATE SET a.title = broaderconcepttitles[0]\n",
    "MERGE (b:ESCO_Skills {uri: broaderconcepturis[1]})\n",
    "ON CREATE SET b.title = broaderconcepttitles[1]\n",
    "MERGE (c:ESCO_Skills {uri: row.conceptUri})\n",
    "ON CREATE SET c.title = row.preferredlabel, c.alt_labels = row.altlabels, c.description = row.description\n",
    "\n",
    "MERGE (a)<-[:SUB_ELEMENT_OF]-(c)\n",
    "MERGE (b)<-[:SUB_ELEMENT_OF]-(c)\n",
    "\",{batchSize:1000, parallel:true, retries: 10}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_transversalskillscollection_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "WHERE row.skillType = 'knowledge' \n",
    "WITH split(row.broaderconceptpt, ' | ') AS broaderconcepttitles, split(row.broaderconcepturi, ' | ') AS broaderconcepturis, row\n",
    "MERGE (a:ESCO_Knowledge {uri: broaderconcepturis[0]})\n",
    "ON CREATE SET a.title = broaderconcepttitles[0]\n",
    "MERGE (b:ESCO_Knowledge {uri: broaderconcepturis[1]})\n",
    "ON CREATE SET b.title = broaderconcepttitles[1]\n",
    "MERGE (c:ESCO_Knowledge {uri: row.conceptUri})\n",
    "ON CREATE SET c.title = row.preferredlabel, c.alt_labels = row.altLabels, c.description = row.description\n",
    "\n",
    "MERGE (a)<-[:SUB_ELEMENT_OF]-(c)\n",
    "MERGE (b)<-[:SUB_ELEMENT_OF]-(c)\n",
    "\n",
    "\",{batchSize:1000, parallel:true, retries: 10}) YIELD operations;\"\"\\n",
    "\n",
    "# prev_num_skills = num_skills\n",
    "# num_skills = run_query(\"\"\"MATCH (a:ESCO_Skills) RETURN count(a) AS num\"\"\['num'][0]\n",
    "# if num_skills <= prev_num_skills:\n",
    "#     interrupt(\"Additional ESCO Skills\", \"there should have been more ESCO Skills created\\n",
    "\n",
    "# prev_num_knowledge = num_knowledge\n",
    "# num_knowledge = run_query(\"\"\"MATCH (a:ESCO_Knowledge) RETURN count(a) AS num\"\"\['num'][0]\n",
    "# if num_knowledge <= prev_num_knowledge:\n",
    "#     interrupt(\"Additional ESCO Knowledge\", \"there should have been more ESCO Skills created\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Add languages to the hierarchy\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_languageskillscollection_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "WHERE row.broaderconceptpt CONTAINS 'languages'\n",
    "MATCH (a:ESCO_Language {title: 'languages'})\n",
    "MERGE (b:ESCO_Language {uri: row.concepturi})\n",
    "ON CREATE SET b.title = row.preferredlabel, b.alt_labels = row.altlabels, b.description = row.description\n",
    "\n",
    "MERGE (a)<-[:SUB_ELEMENT_OF]-(c)\n",
    "\",{batchSize:1000, parallel:true, retries: 10}) YIELD operations;\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_languageskillscollection_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "WHERE row.broaderconceptpt <> 'languages | language' AND row.broaderconceptpt <> 'language | languages'\n",
    "MATCH (a:ESCO_Language {title: row.broaderconceptpt})\n",
    "MERGE (b:ESCO_Language {uri: row.concepturi})\n",
    "ON CREATE SET b.title = row.preferredlabel, b.alt_labels = row.altlabels, b.description = row.description\n",
    "\n",
    "MERGE (a)<-[:SUB_ELEMENT_OF]-(b)\n",
    "\",{batchSize:1000, parallel:true, retries: 10}) YIELD operations;\"\"\\n",
    "\n",
    "# check_queries = []\n",
    "# check_queries.append(\"\"\"MATCH p=(:ESCO_Language {title: 'languages'})<-[:SUB_ELEMENT_OF]-(:ESCO_Language) RETURN count(p) AS num\"\"\\n",
    "# check_queries.append(\"\"\"MATCH p=(:ESCO_Language)<-[:SUB_ELEMENT_OF]-(:ESCO_Language) RETURN count(p) AS num\"\"\\n",
    "# for cq in check_queries:\n",
    "#     results = run_query(cq)['num'][0]\n",
    "#     if results <= 1:\n",
    "#         interrupt(\"Additional ESCO KSATTs\", \"there should be more ESCO KSATTs or relationships between them\\n",
    "\n",
    "# prev_num_language = num_language\n",
    "# num_language = run_query(\"\"\"MATCH (a:ESCO_Language) RETURN count(a) AS num\"\"\['num'][0]\n",
    "# if num_language <= prev_num_language:\n",
    "#     interrupt(\"Additional ESCO Languages\", \"there should have been more ESCO Languages created\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Add additional skills and knowledge\\n",
    "# TODO: These don't really connect to any other parts of the hierarchy as far as I could figure out.\n",
    "\n",
    "# add other skills\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_skills_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "WHERE row.skilltype = 'skill/competence'\n",
    "MERGE (a:ESCO_Skills {uri: row.concepturi})\n",
    "ON CREATE SET a.title = row.preferredlabel, a.alt_title = row.altlabels, a.description = row.description\n",
    "\",{batchSize:1000, parallel:false}) YIELD operations;\"\"\\n",
    "\n",
    "# add other knowledge\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_skills_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "WHERE row.skilltype = 'knowledge'\n",
    "MERGE (a:ESCO_Knowledge {uri: row.concepturi})\n",
    "ON CREATE SET a.title = row.preferredlabel, a.alt_title = row.altlabels, a.description = row.description\n",
    "\",{batchSize:1000, parallel:false}) YIELD operations;\"\"\\n",
    "\n",
    "# prev_num_skills = num_skills\n",
    "# num_skills = run_query(\"\"\"MATCH (a:ESCO_Skills) RETURN count(a) AS num\"\"\['num'][0]\n",
    "# if num_skills <= prev_num_skills:\n",
    "#     interrupt(\"Additional ESCO Skills\", \"there should have been more ESCO Skills created\\n",
    "\n",
    "# prev_num_knowledge = num_knowledge\n",
    "# num_knowledge = run_query(\"\"\"MATCH (a:ESCO_Knowledge) RETURN count(a) AS num\"\"\['num'][0]\n",
    "# if num_knowledge <= prev_num_knowledge:\n",
    "#     interrupt(\"Additional ESCO Knowledge\", \"there should have been more ESCO Skills created\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Remove circular relationships AND add IDs based off of URIs\\n",
    "esco_labels = ['ESCO_Skills', 'ESCO_Knowledge', 'ESCO_Language', 'ESCO_Attitudes_Values']\n",
    "for el in esco_labels:\n",
    "    results = run_reg_query(\"\"\"MATCH (a:{})-[r:SUB_ELEMENT_OF]-(a) DELETE r RETURN count(r) AS num\"\"\".format(el))['num'][0]\n",
    "    print(\"Deleted\", results, \"relationships.\\n",
    "    run_reg_query(\"\"\"\n",
    "        MATCH (a:{})\n",
    "        WITH split(a.uri, '/')[5] AS id_from_uri, a\n",
    "        SET a.key = id_from_uri\n",
    "    \"\"\".format(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Create occupations and groups\\n",
    "# create occupation groups - don't relate them yet\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_iscogroups_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "WITH row, split(row.concepturi, 'C')[1] AS code\n",
    "MERGE (a:ESCO_Occupation_Group {uri: row.concepturi})\n",
    "ON CREATE SET a.key = code, a.title = row.preferredlabel, a.alt_labels = row.altlabels, a.description = row.description\n",
    "\",{batchSize:1000, parallel:true, retries: 10}) YIELD operations;\"\"\\n",
    "\n",
    "### WHAT THIS DOES ###\n",
    "# find occupation groups where \n",
    "    # parent and child ids are not the same (e.g. 12, 121, 1213)\n",
    "    # parent id is one less number in length than the child id (e.g. 12 is one number shorter than 121, same for 121 and 1213)\n",
    "    # the parent id and a substring of the child id match\n",
    "        # the substring of the child id starts at the first number in the id and is the length of the parent id\n",
    "        # so in effect it's matching the substring of the child id that should be equal to the parent id \n",
    "        # e.g. 1213 child, 121 parent -> 1213 starts at first '1' and goes through '121' because that's the length of the parent 121\n",
    "# create a relationship where the parent and child ids match up\n",
    "run_reg_query(\"\"\"\n",
    "    MATCH (a:ESCO_Occupation_Group), (b:ESCO_Occupation_Group)\n",
    "    WHERE a.key <> b.key AND size(b.key)-1 = size(a.key) AND a.key = substring(b.key, 0, size(a.key))\n",
    "    MERGE (a)<-[:SUB_ELEMENT_OF]-(b)\n",
    "\"\"\\n",
    "\n",
    "# create occupations and assign them to occupation groups\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///esco/trusted_occupations_en.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ESCO_Occupation_Group {key: row.iscogroup})\n",
    "MERGE (b:ESCO_Occupation {uri: row.concepturi})\n",
    "SET b.title = row.preferredlabel, b.alt_labels = row.altlabels, b.description = row.description, b.key = row.code\n",
    "MERGE (a)<-[:IN_GROUP]-(b)\n",
    "\",{batchSize:1000, parallel:false, retries: 10}) YIELD operations;\"\"\\n",
    "\n",
    "run_reg_query(\"\"\"\n",
    "    MATCH (a:ESCO_Occupation), (b:ESCO_Occupation)\n",
    "    WHERE a.key <> b.key AND (size(b.key)-2 = size(a.key) OR size(b.key)-3 = size(a.key)) AND a.key = substring(b.key, 0, size(a.key))\n",
    "    MERGE (a)<-[:SUB_ELEMENT_OF]-(b)\n",
    "\"\"\\n",
    "\n",
    "results1 = run_reg_query(\"\"\"MATCH p=(a:ESCO_Occupation_Group)<--(b:ESCO_Occupation_Group) WHERE a.key<>b.key AND size(b.key)-1 = size(a.key) AND b.key CONTAINS a.key RETURN count(p) AS num\"\"\['num'][0]\n",
    "results2 = run_reg_query(\"\"\"MATCH p=(:ESCO_Occupation_Group)<--(:ESCO_Occupation_Group)<--(:ESCO_Occupation_Group)<--(:ESCO_Occupation_Group)<--(:ESCO_Occupation)<--(:ESCO_Occupation)<--(:ESCO_Occupation)<--(:ESCO_Occupation) RETURN count(p) AS num\"\"\['num'][0]\n",
    "if results1 <= 1 or results2 <= 1:\n",
    "    interrupt(\"ESCO Occupations and Groups\", \"there should be relationships\\n",
    "\n",
    "results = run_reg_query(\"\"\"MATCH (a:ESCO_Occupation) WHERE NOT (:ESCO_Occupation)<--(a) AND NOT (:ESCO_Occupation_Group)<--(a) RETURN count(a) AS num\"\"\['num'][0]\n",
    "if results >= 1:\n",
    "    interrupt(\"ESCO Occupations\", \"there should not be any ESCO Occupations not related to another occupation or group\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: try relating skills and occupations using a bit of web scraping\n",
    "\n",
    "# base_url = 'https://ec.europa.eu/esco/portal/occupation?uri=http%3A%2F%2Fdata.europa.eu%2Fesco%2Foccupation%2F00030d09-2b3a-4efd-87cc-c4ea39d27c34&conceptLanguage=en&full=true#&uri='\n",
    "# uri_list = run_query(\"\"\"MATCH (a:ESCO_Occupation) RETURN a.uri AS uri\"\"\['uri']\n",
    "# for uri in uri_list:\n",
    "#     try:\n",
    "#         r = requests.get(base_url+uri)\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print('ERROR: could not access uri.')\n",
    "#         raise SystemExit(e)\n",
    "#     soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "#     # https://stackoverflow.com/questions/5690686/using-nextsibling-from-beautifulsoup-outputs-nothing explains why you have to double up\n",
    "#     header_list = [\"Essential Knowledge\"]\n",
    "#     # header_list = [\"Essential Knowledge\", \"Essential skills and competences\", \"Optional skills and competences\", \"Optional Knowledge\"]\n",
    "#     for h in header_list:\n",
    "#         # clear uri list and decide on relationship and label type depending on header\n",
    "#         ksatt_uris = []\n",
    "#         if 'essential' in h.lower():\n",
    "#             rel = 'ESSENTIAL_FOR'\n",
    "#         else:\n",
    "#             rel = 'OPTIONAL_FOR'\n",
    "#         if 'knowledge' in h.lower():\n",
    "#             label = 'ESCO_Knowledge'\n",
    "#         else:\n",
    "#             label = 'ESCO_Skills'\n",
    "#         # look for that header\n",
    "#         h2 = soup.find(\"h2\", text=h)\n",
    "#         # if the header actually exists\n",
    "#         if h2 is not None:\n",
    "#             ul = h2.nextSibling.nextSibling # find the tag after the header - will be the unordered list keeping all the other ksatt links underneath\n",
    "#             for li in ul.find_all('li'): # find all the nested links\n",
    "#                 a = li.find('a') # grab the individual pieces\n",
    "#                 ksatt_uris.append(a.attrs['href']) # grab the link and add to our list of uris\n",
    "#             # once all the uris are collected for a header, create relationships of the right type between the occupation and the KSATT with that URI! \n",
    "#             for ksatt_uri in ksatt_uris:\n",
    "#                 run_query(\"\"\"\n",
    "#                     MATCH (a:ESCO_Occupation {{uri: '{}'}})\n",
    "#                     MATCH (b:{} {{uri: '{}'}})\n",
    "#                     MERGE (a)<-[:{}]-(b)\n",
    "#                 \"\"\".format(uri, label, ksatt_uri, rel))\n",
    "\n",
    "\n",
    "# # took 40m to get through 827/2942 ESCO_Occupations - essential AND optional knowledge/skills\n",
    "\n",
    "# # took ___ to get through__________ ESCO_Occupations - essential knowledge only\n",
    "\n",
    "# #28 per min avg\n",
    "# # with 2942 = 100 min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload NICE Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NICE Framework Background  \n",
    "- NIST (National Institute of Standards and Technology), NICCS (National Initiative for Cybersecurity Careers and Studies), OPM (Office of Personnel Management) all have a copy of the NICE (National Initiative for Cybersecurity Education) framework, which was developed by NICE. \n",
    "- Straight from OPM: OPM and DHS during the early stages of its collaborative endeavors co-led efforts to identify the cybersecurity workforce. With the direct engagement of over 20 Federal departments and agencies, and numerous public and private organizations, the National Initiative for Cybersecurity Education (NICE) developed the National Cybersecurity Workforce Framework (the Framework) to define cybersecurity work and lay a foundation for cybersecurity workforce efforts. The NICE Framework provides a common language and taxonomy, defines specialty areas and KSAs/competencies, and codifies talent.\n",
    "- NICCS has NICE embedded in their website https://niccs.cisa.gov/workforce-development/cyber-security-workforce-framework \n",
    "- NIST has an excel spreadsheet https://www.nist.gov/itl/applied-cybersecurity/nice/nice-framework-resource-center/workforce-framework-cybersecurity-nice \n",
    "- OPM has it in a PDF https://www.opm.gov/policy-data-oversight/classification-qualifications/reference-materials/interpretive-guidance-for-cybersecurity-positions.pdf \n",
    "Other Resources  \n",
    "  - https://csrc.nist.gov/projects/olir/focal-document-templates\n",
    "  - https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.04162018.pdf\n",
    "  - https://www.nist.gov/cyberframework/framework\n",
    "  - https://niccs.cisa.gov/workforce-development/cyber-security-workforce-framework\n",
    "  - Withdrawn: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-181.pdf\n",
    "  - Superseded by: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-181r1.pdf\n",
    "  - https://csrc.nist.gov/publications/detail/sp/800-181/rev-1/final\n",
    "  - https://niccs.cisa.gov/workforce-development/cyber-career-pathways\n",
    "  - https://niccs.cisa.gov/workforce-development\n",
    "OPM Cybersecurity Codes are different from regular OPM Series Codes  \n",
    "  - Grab Reference Spreadsheet https://www.nist.gov/itl/applied-cybersecurity/nice/nice-framework-resource-center/workforce-framework-cybersecurity-nice\n",
    "  - Table of Contents has OPM codes, but these are not the same as the ones from OPM - they're 3 digits instead of the GS-XXXX structure. Researching into it, OPM actually has separate codes for these cybersecurity roles https://dw.opm.gov/datastandards/referenceData/2273/current?category=&q=cybersecurity. \n",
    "  - Tried to figure out more about these cybersecurity-specific codes\n",
    "      - https://www.opm.gov/policy-data-oversight/classification-qualifications/classifying-general-schedule-positions/#url=Standards\n",
    "      - made sense to find cybersecurity under 2200 IT Group so went digging into that PDF https://www.opm.gov/policy-data-oversight/classification-qualifications/classifying-general-schedule-positions/standards/2200/gs2200a.pdf\n",
    "      - Searched for 2210 and found the IT Cybersecurity Specialist role, this additional document was linked https://www.opm.gov/policy-data-oversight/classification-qualifications/reference-materials/interpretive-guidance-for-cybersecurity-positions.pdf \n",
    "      - In that Interpretive Guidance for Cybersecurity Positions, there was the NICE framework and more explanations about these cybersecurity codes\n",
    "      - Hard to tell if they are supposed to be under the 2210 focus, or are just completely separate. \n",
    "          - Don't think it's just 2210 because a section of the PDF includes that they also have overlap with the 0855, 0854, and 0391 series. \n",
    "PDW has Cybersecurity codes that match with the ones in the NICE Framework, but...  \n",
    "  - These codes are actually results of something David did a couple years ago.\n",
    "  - He compared position descriptions of employees with the descriptions for the cybersecurity codes and recommended the top 3 codes based on the doc2vec results. \n",
    "  - Main issues with this now:\n",
    "      - Is it being maintained/updated live?\n",
    "      - The position descriptions used at that time were more detailed than the ones we have now - would be comparing different results. \n",
    "  - Possible solution might be that we infer the cybersecurity code based on KSATT overlap, rather than code crosswalking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Use table of contents to establish and relate NICE Categories, Areas and Workroles\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///nice/trusted_nice_areas_and_roles.csv' AS row RETURN row\",\n",
    "\"\n",
    "MERGE (a:NICE_Category {title: toLower(row.nice_category_title), acronym: row.nice_category_acronym, description: toLower(row.nice_category_description)})\n",
    "\",{batchSize:10000}) YIELD operations;\n",
    "\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///nice/trusted_nice_areas_and_roles.csv' AS row RETURN row\",\n",
    "\"\n",
    "MERGE (a:NICE_Area {title: toLower(row.nice_specialty_area_title), acronym: row.nice_specialty_area_acronym, description: toLower(row.nice_specialty_area_description)})\n",
    "\",{batchSize:10000}) YIELD operations;\n",
    "\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///nice/trusted_nice_areas_and_roles.csv' AS row RETURN row\",\n",
    "\"\n",
    "WITH row, split(row.work_role_id, '-') AS work_role_id_components\n",
    "MERGE (a:NICE_Workrole {key: row.work_role_id, key_1: work_role_id_components[0], key_2: work_role_id_components[1], key_3: work_role_id_components[2], title: toLower(row.work_role), description: toLower(row.work_role_description)})\n",
    "WITH row, a\n",
    "MATCH (b:NICE_Area {acronym: a.key_2})\n",
    "MATCH (c:NICE_Category {acronym: a.key_1})\n",
    "MERGE (a)-[:IN_AREA]->(b)\n",
    "MERGE (b)-[:IN_CATEGORY]->(c)\n",
    "\",{batchSize:10000}) YIELD operations;\n",
    "\"\"\\n",
    "\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///nice/trusted_nice_areas_and_roles.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:NICE_Workrole {key: row.work_role_id})\n",
    "MERGE (b:OPM_Cybersecurity_Category {key: row.opm_code_fed_use})\n",
    "MERGE (a)-[:IN_CATEGORY]->(b)\n",
    "\",{batchSize:10000}) YIELD operations;\n",
    "\"\"\\n",
    "\n",
    "results = run_reg_query(\"\"\"MATCH a=(:NICE_Category)<-[:IN_CATEGORY]-(:NICE_Area)<-[:IN_AREA]-(:NICE_Workrole)-[:IN_CATEGORY]->(:OPM_Cybersecurity_Category) RETURN count(a) AS num\"\"\['num'][0]\n",
    "if results == 0:\n",
    "    interrupt(\"NICE Categories & Areas & Workroles\", \"there should be connected nodes\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Add NICE KSATTs and relate them to NICE Workroles\\n",
    "# add NICE_Tasks and connect to NICE_Workrole nodes\n",
    "# don't add KSA id because there are KSAs with the same title but different IDs, and the IDs don't matter (no hierarchy or context)\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///nice/trusted_master_ksatt_list_mapped.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:NICE_Workrole {key: row.work_role_id})\n",
    "MERGE (b:NICE_KSAs {title: toLower(row.title)})\n",
    "ON CREATE SET b.key = row.id\n",
    "MERGE (b)-[:FOUND_IN]->(a)\n",
    "\",{batchSize:10000}) YIELD operations;\n",
    "\"\"\\n",
    "\n",
    "# set NICE_KSAs labels to be more specific\n",
    "run_reg_query(\"\"\"MATCH (a:NICE_KSAs) WHERE a.key CONTAINS 'K' SET a:NICE_Knowledge REMOVE a:NICE_KSAs, a.key\"\"\\n",
    "run_reg_query(\"\"\"MATCH (a:NICE_KSAs) WHERE a.key CONTAINS 'S' SET a:NICE_Skills REMOVE a:NICE_KSAs, a.key\"\"\\n",
    "run_reg_query(\"\"\"MATCH (a:NICE_KSAs) WHERE a.key CONTAINS 'A' SET a:NICE_Abilities REMOVE a:NICE_KSAs, a.key\"\"\\n",
    "run_reg_query(\"\"\"MATCH (a:NICE_KSAs) WHERE a.key CONTAINS 'T' SET a:NICE_Tasks REMOVE a:NICE_KSAs, a.key\"\"\\n",
    "check_queries = []\n",
    "check_queries.append(\"\"\"MATCH a=(:NICE_Knowledge)-[:FOUND_IN]->(:NICE_Workrole) RETURN count(a) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH a=(:NICE_Skills)-[:FOUND_IN]->(:NICE_Workrole) RETURN count(a) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH a=(:NICE_Abilities)-[:FOUND_IN]->(:NICE_Workrole) RETURN count(a) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH a=(:NICE_Tasks)-[:FOUND_IN]->(:NICE_Workrole) RETURN count(a) AS num\"\"\\n",
    "for cq in check_queries:\n",
    "    results = run_reg_query(cq)['num'][0]\n",
    "    if results == 0:\n",
    "        interrupt(\"NICE Workroles & KSATTs\", \"there should be connected nodes\"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosswalk OPM and ONET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Add in the OPM Series to ONET Occupation Crosswalk\\n",
    "\n",
    "# create OPM Series nodes\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///trusted_opm_onet_crosswalk.csv' AS row RETURN row\",\n",
    "\"\n",
    "MERGE (:OPM_Series {key: row.opm_series_number, title: row.opm_series_title})\n",
    "\",{batchSize:1000}) YIELD operations;\"\"\\n",
    "\n",
    "# create relationship between occupations and opm key's if they're in the right row\n",
    "run_apoc_query(\"\"\"CALL apoc.periodic.iterate(\n",
    "\"LOAD CSV WITH HEADERS FROM 'file:///trusted_opm_onet_crosswalk.csv' AS row RETURN row\",\n",
    "\"\n",
    "MATCH (a:ONET_Occupation), (b:OPM_Series {key: row.opm_series_number})\n",
    "WHERE a.key CONTAINS(row.`2010_soc_code`)\n",
    "MERGE (a)-[r:IN_OPM {census_code: row.`2010_eeo_tabulation_census_code`, census_title: toLower(row.`2010_eeo_tabulation_census_occupation_title`)}]->(b)\n",
    "\",{batchSize:1000}) YIELD operations;\"\"\\n",
    "\n",
    "result = run_reg_query(\"\"\"MATCH a=(:ONET_Occupation)-[:IN_OPM]->(:OPM_Series) RETURN count(a) AS num\"\"\['num'][0]\n",
    "print(result)\n",
    "if result == 0:\n",
    "    interrupt(\"Occupations to OPM Series\", \"should be some relationships but there's none\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Equate OPM Series to ONET Occupations for specific SOC Codes\\n",
    "\n",
    "run_reg_query(\"\"\"MATCH (a:ONET_Occupation {key: '17-2071.00'}), (b:OPM_Series) WHERE b.key CONTAINS(\"855\\n",
    "MERGE (a)-[:IN_OPM {census_code: '1410', census_title: toLower('ELECTRICAL & ELECTRONIC ENGINEERS')}]->(b);\"\"\\n",
    "\n",
    "run_reg_query(\"\"\"MATCH (a:ONET_Occupation {key: '17-2072.00'}), (b:OPM_Series) WHERE b.key CONTAINS(\"855\\n",
    "MERGE (a)-[:IN_OPM {census_code: '1410', census_title: toLower('ELECTRICAL & ELECTRONIC ENGINEERS')}]->(b);\"\"\\n",
    "\n",
    "run_reg_query(\"\"\"MATCH (a:ONET_Occupation), (b:OPM_Series) WHERE a.key CONTAINS('17-206') AND b.key CONTAINS(\"854\\n",
    "MERGE (a)-[:IN_OPM {census_code: '1400', census_title: toLower('COMPUTER HARDWARE ENGINEERS')}]->(b);\"\"\\n",
    "\n",
    "# TODO: was 15-1111 but no SOC codes match that\n",
    "# run_query(\"\"\"MATCH (a:ONET_Occupation), (b:OPM_Series) WHERE a.key CONTAINS('15-1111') AND b.key CONTAINS(\"1550\\n",
    "# MERGE (a)-[:IN_OPM {census_code: '1005', census_title: toLower('COMPUTER & INFORMATION RESEARCH SCIENTISTS')}]->(b);\"\"\\n",
    "run_reg_query(\"\"\"MATCH (a:ONET_Occupation), (b:OPM_Series) WHERE a.key CONTAINS('15-1') AND b.key CONTAINS(\"1550\\n",
    "MERGE (a)-[:IN_OPM {census_code: '1005', census_title: toLower('COMPUTER & INFORMATION RESEARCH SCIENTISTS')}]->(b);\"\"\\n",
    "\n",
    "run_reg_query(\"\"\"MATCH (a:ONET_Occupation), (b:OPM_Series) WHERE a.key CONTAINS('15-1') AND b.key CONTAINS('2210')\n",
    "MERGE (a)-[:IN_OPM {census_code: '1050', census_title: toLower('COMPUTER SUPPORT SPECIALISTS')}]->(b);\"\"\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check the OPM to ONET Crosswalk queries\\n",
    "\n",
    "check_queries = []\n",
    "check_queries.append(\"\"\"MATCH p=(a:ONET_Occupation {key: '17-2071.00'})-[:IN_OPM]->(b:OPM_Series) WHERE b.key CONTAINS(\"855\ RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(a:ONET_Occupation {key: '17-2072.00'})-[:IN_OPM]->(b:OPM_Series) WHERE b.key CONTAINS(\"855\ RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(a:ONET_Occupation)-[:IN_OPM]->(b:OPM_Series) WHERE a.key CONTAINS('17-206') AND b.key CONTAINS(\"854\ RETURN count(p) AS num\"\"\\n",
    "# TODO: same here\n",
    "# check_queries.append(\"\"\"MATCH p=(a:ONET_Occupation)-[:IN_OPM]->(b:OPM_Series) WHERE a.key CONTAINS('15-1111') AND b.key CONTAINS(\"1550\ RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(a:ONET_Occupation)-[:IN_OPM]->(b:OPM_Series) WHERE a.key CONTAINS('15-1') AND b.key CONTAINS(\"1550\ RETURN count(p) AS num\"\"\\n",
    "check_queries.append(\"\"\"MATCH p=(a:ONET_Occupation)-[:IN_OPM]->(b:OPM_Series) WHERE a.key CONTAINS('15-1') AND b.key CONTAINS(\"2210\ RETURN count(p) AS num\"\"\\n",
    "\n",
    "for cq in check_queries:\n",
    "    results = run_reg_query(cq)['num'][0]\n",
    "    if results <= 0:\n",
    "        interrupt(\"ONET Occupation to OPM Crosswalk\", \"there should be specific relationships\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('3.10.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "515d34f2bce719b596f3fc388ecb095fb9a637c1e6a8f10b32ddbd0646bb41aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
